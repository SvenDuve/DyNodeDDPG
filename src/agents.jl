
function trainAgent(pms::Parameter, agent::DDPGAgent)
    #@show pms
    println("Welcome to this function.")
    gym = pyimport("gym")
    global env = gym.make(pms.environment)

    global p = resetParameters(pms)

    # set buffer
    global ùíü = []

    # get critic

    global QŒ∏ = setNetwork(Critic())
    global QŒ∏‚Ä≤ = deepcopy(QŒ∏)

    # get actor

    global Œºœï = setNetwork(Actor())
    global Œºœï‚Ä≤ = deepcopy(Œºœï)

    # copy actor
    #@show QŒ∏, Œºœï



    # set optimiszers

    global opt_critic = Optimise.Adam(p.Œ∑_critic)
    global opt_actor = Optimise.Adam(p.Œ∑_actor)

    # set training conditions

    # global ou = OrnsteinUhlenbeck(0.0f0, 0.15f0, 0.2f0, [0.0f0])

    scores = zeros(100)
    e = 1
    idx = 1

    while e <= p.max_episodes

        ep = Episode(env, AgentPolicy(), p)()


        for (s, a, r, s‚Ä≤, t) in ep.episode
            remember(p.mem_size, s, a, r, s‚Ä≤, t)
            p.frames += 1

            if length(ùíü) >= p.batch_size# && œÄ.train
                train(agent)
            end

        end

        scores[idx] = ep.total_reward
        idx = idx % 100 + 1
        avg = mean(scores)
        println("Episode: $e | Score: $(ep.total_reward) | Avg score: $avg | Frames: $(p.frames)")
        e += 1

    end

    #@show ep.total_reward

    # loop while some conditions

    # interact with the environment by generating some Episode
    # store episode in buffer

    # create a minibatch from the buffer

    # get all parameters for the loss functions

    # Do gradient ascent/ descent on the loss functions

    # update target for critic

    # update target for actor

    return -1

end #trainAgent


