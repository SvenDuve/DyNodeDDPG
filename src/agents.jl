
function trainAgent(agent::DDPGAgent, pms::Parameter)
    #@show pms
    println("Welcome to this function.")
    gym = pyimport("gym")
    global env = gym.make(pms.environment)

    global p = resetParameters(pms)

    # set buffer
    global ğ’Ÿ = []

    # get critic

    global QÎ¸ = setNetwork(Critic())
    global QÎ¸â€² = deepcopy(QÎ¸)

    # get actor

    global Î¼Ï• = setNetwork(Actor())
    global Î¼Ï•â€² = deepcopy(Î¼Ï•)

    # copy actor
    #@show QÎ¸, Î¼Ï•



    # set optimiszers

    global opt_critic = Optimise.Adam(p.Î·_critic)
    global opt_actor = Optimise.Adam(p.Î·_actor)

    # set training conditions

    # global ou = OrnsteinUhlenbeck(0.0f0, 0.15f0, 0.2f0, [0.0f0])

    scores = zeros(100)
    e = 1
    idx = 1

    while e <= p.max_episodes

        ep = Episode(env, agent, p)()


        for (s, a, r, sâ€², t) in ep.episode
            remember(p.mem_size, s, a, r, sâ€², t)
            p.frames += 1

            if length(ğ’Ÿ) >= p.batch_size# && Ï€.train
                train(agent)
            end

        end

        scores[idx] = ep.total_reward
        idx = idx % 100 + 1
        avg = mean(scores)
        println("Episode: $e | Score: $(ep.total_reward) | Avg score: $avg | Frames: $(p.frames)")
        e += 1

    end

    #@show ep.total_reward

    # loop while some conditions

    # interact with the environment by generating some Episode
    # store episode in buffer

    # create a minibatch from the buffer

    # get all parameters for the loss functions

    # Do gradient ascent/ descent on the loss functions

    # update target for critic

    # update target for actor

    return -1

end #trainAgent



function dyNode(m::DyNodeModel, pms::Parameter)

    # To Do's:
    # to set up dynode_batch_size -> 64 in the paper

    # interactions with the real World


    gym = pyimport("gym")
    global env = gym.make(pms.environment)
    global p = resetParameters(pms)


    # set buffer
    global ğ’Ÿ = []

    # global fÎ¸ = setNode(m, p)
    global fÎ¸ = setNetwork(m) # Code up a Network that will be solved with euler steps
    global RÏ• = setNetwork(Rewards())

    # global optR = Flux.setup(Optimise.Adam(), RÏ•)#` and pass this `opt` to `train!
    # global opt_model = Optimise.Adam(0.005)
    # global opt_reward = Optimise.Adam(0.005)

    for i in 1:p.Sequences
        ep = Episode(env, m, p)()
        for (s, a, r, sâ€², t) in ep.episode
            remember(p.mem_size, s, a, r, sâ€², t)
        end

        train(m)
        if i % 10 == 0
            println("Iteration $i")
        end
    end

    return fÎ¸, RÏ•, ğ’Ÿ

end