
function trainAgent(agent::DDPGAgent, pms::Parameter)
    #@show pms
    println("Welcome to this function.")
    gym = pyimport("gym")
    global env = gym.make(pms.environment)

    global p = resetParameters(pms)

    # set buffer
    global ùíü = []

    # get critic

    global QŒ∏ = setNetwork(Critic())
    global QŒ∏‚Ä≤ = deepcopy(QŒ∏)

    # get actor

    global Œºœï = setNetwork(Actor())
    global Œºœï‚Ä≤ = deepcopy(Œºœï)

    # copy actor
    #@show QŒ∏, Œºœï



    # set optimiszers

    # global opt_critic = Optimise.Adam(p.Œ∑_critic)
    # global opt_actor = Optimise.Adam(p.Œ∑_actor)

    # set training conditions

    # global ou = OrnsteinUhlenbeck(0.0f0, 0.15f0, 0.2f0, [0.0f0])

    scores = zeros(100)
    e = 1
    idx = 1

    while e <= p.max_episodes

        ep = Episode(env, agent, p)()


        for (s, a, r, s‚Ä≤, t) in ep.episode
            remember(p.mem_size, s, a, r, s‚Ä≤, t)
            p.frames += 1

            if length(ùíü) >= p.batch_size# && œÄ.train
                train(agent)
            end

        end

        scores[idx] = ep.total_reward
        idx = idx % 100 + 1
        avg = mean(scores)
        println("Episode: $e | Score: $(ep.total_reward) | Avg score: $avg | Frames: $(p.frames)")
        e += 1

    end

    return -1

end #trainAgent



function dyNode(m::DyNodeModel, pms::Parameter)

    # To Do's:
    # to set up dynode_batch_size -> 64 in the paper

    # interactions with the real World


    gym = pyimport("gym")
    global env = gym.make(pms.environment)
    global p = resetParameters(pms)


    # set buffer
    global ùíü = []

    # global fŒ∏ = setNode(m, p)
    global fŒ∏ = setNetwork(m) # Code up a Network that will be solved with euler steps
    global Rœï = setNetwork(Rewards())


    for i in 1:p.Sequences
        ep = Episode(env, m, p)()
        for (s, a, r, s‚Ä≤, t) in ep.episode
            remember(p.mem_size, s, a, r, s‚Ä≤, t)
        end

        model_loss, reward_loss = train(m)
        # alt_train(m)
        if i % 10 == 0
            println("Iteration $i")
        end
        append!(p.model_loss, model_loss)
        append!(p.reward_loss, reward_loss)
    end
    return p, fŒ∏, Rœï
end