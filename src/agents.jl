
function trainAgent(agent::DDPGAgent, pms::Parameter)
    #@show pms
    println("Welcome to this function.")
    gym = pyimport("gym")
    global env = gym.make(pms.environment)

    global p = resetParameters(pms)

    setNoise(p)
    # if p.noise == "gaussian"
    #     global noise = GaussianNoise(0.0f0, 0.1f0)
    # else
    #     global noise = OrnsteinUhlenbeck(0.0f0, 0.15f0, 0.5f0, [0.0f0])
    # end

    # set buffer
    global ùíü = []

    # get critic

    global QŒ∏ = setNetwork(Critic())
    global QŒ∏‚Ä≤ = deepcopy(QŒ∏)

    # get actor

    global Œºœï = setNetwork(Actor())
    global Œºœï‚Ä≤ = deepcopy(Œºœï)


    scores = zeros(100)
    e = 1
    idx = 1

    while e <= p.max_episodes

        ep = Episode(env, agent, p)()


        for (s, a, r, s‚Ä≤, t) in ep.episode
            remember(p.mem_size, s, a, r, s‚Ä≤, t)
            p.frames += 1

            # if length(ùíü) >= p.train_start# && œÄ.train
            if p.frames >= p.train_start# && œÄ.train
                train(agent)
            end
            
        end
        

        scores[idx] = ep.total_reward
        idx = idx % 100 + 1
        avg = mean(scores)
        if (e-1) % 10 == 0
            println("Episode: $e | Score: $(ep.total_reward) | Avg score: $avg | Frames: $(p.frames)")
        end
        e += 1

        append!(p.total_rewards, ep.total_reward)

    end

    return Œºœï, p

end #trainAgent



function dyNode(m::DyNodeModel, pms::Parameter)

    # To Do's:
    # to set up dynode_batch_size -> 64 in the paper

    # interactions with the real World


    gym = pyimport("gym")
    global env = gym.make(pms.environment)
    global p = resetParameters(pms)

    #setNoise(p)

    # set buffer
    global ùíü = []

    # global fŒ∏ = setNode(m, p)
    global fŒ∏ = setNetwork(m) # Code up a Network that will be solved with euler steps
    global Rœï = setNetwork(Rewards())


    for i in 1:p.Sequences
        ep = Episode(env, m, p)()
        for (s, a, r, s‚Ä≤, t) in ep.episode
            remember(p.mem_size, s, a, r, s‚Ä≤, t)
        end

        model_loss, reward_loss = train(m)
        # alt_train(m)
        if i % 10 == 0
            println("Iteration $i")
        end
        append!(p.model_loss, model_loss)
        append!(p.reward_loss, reward_loss)
    end
    return p, fŒ∏, Rœï
end





function NODEAgent(m::NodeModel, pms::Parameter)

    # To Do's:
    # to set up dynode_batch_size -> 64 in the paper

    # interactions with the real World


    gym = pyimport("gym")
    global env = gym.make(pms.environment)
    global p = resetParameters(pms)

    #setNoise(p)
    #@show noise

    # set buffer
    global ùíü = []

    global fŒ∏ = setNode(m, p)
    global Rœï = setNetwork(Rewards())


    for i in 1:p.Sequences
        ep = Episode(env, m, p)()
        for (s, a, r, s‚Ä≤, t) in ep.episode
            remember(p.mem_size, s, a, r, s‚Ä≤, t)
        end

        model_loss, reward_loss = train(m)
        if i % 10 == 0
            println("Iteration $i")
        end
        append!(p.model_loss, model_loss)
        append!(p.reward_loss, reward_loss)
    end
    
    return p, fŒ∏, Rœï

end


